{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021_06_11_수업_자연어.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPvh5ixQa6vLGIC5B5aBes0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/risker93/Hello_World/blob/main/2021_06_11_%EC%88%98%EC%97%85_%EC%9E%90%EC%97%B0%EC%96%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTjO5bvB6Ols"
      },
      "source": [
        "자연어는 인간이 의사소통을 하는 언어\n",
        "\n",
        "자연어 생성 nlg\n",
        "자연어 이해 nlu\n",
        "\n",
        "nlg, nlu는 챗봇의 핵심 구성 요소\n",
        "\n",
        "형태소를 분석하고, 토픽을추측, 분석해서 , 번역\n",
        "\n",
        "코퍼스란? \n",
        "말뭉치 라고도 하며 여러 문장으로 구성된 문서의 집합\n",
        "\n",
        "텍스트 전처리 > 네트워크 모델 설계> 모델 훈련> 모델 예측 및 평가.\n",
        "\n",
        "텍스트 전처리는 정제, 불용어제거, 어간추출, 토큰화 및 문서 표현\n",
        "\n",
        "불용어 = stop word =  a, the 같은 것들\n",
        "\n",
        "정제(Cleaning) 특수문자등을 제거\n",
        "\n",
        "불용어 제거(Stop word Elimination) 전치사, 관사 등 문장이나 문서의 특징을 표현하는데 불필요한 단어를 제거하는 단계\n",
        "\n",
        "어간추출: 단어의 기본 형태를 추출하는 단계 \n",
        "\n",
        "토큰화: 코퍼스(Corpus)에서 분리자(Separator)를 포함하지 않는 \n",
        "\n",
        "문서표현 : 주어진 문서나 문장을 하나의 백터로 표현하는 단계\n",
        "\n",
        "텍스트 전처리 이후 모델설계를 함\n",
        "\n",
        "인공지능 자연어처리 \n",
        "형태소 분석개념\n",
        "\n",
        "형태소란? : 의미단위\n",
        "형태소 분석 : 의미 단위의 형태소를 분석\n",
        "\n",
        "형태소 예시 :  보/았/습니다.\n",
        "\n",
        "자연어 처리는 일반적으로 형태소 분석, 구문 분석, 의미분석, 담화분석 등의 과정으로 수행\n",
        "\n",
        "Morphological > Syntactic > Semantic > Discourse\n",
        "\n",
        "pos = part of speech\n",
        "\n",
        "형태소 분석기 의 한계\n",
        "\n",
        "주어' 나는' \t술어 '나는'  : 이 두개를 구분 불가\n",
        "의미분석\n",
        "구문 분석 결과에 해석을 가미 > 문장의 의미를 분석하는 과정\n",
        "\n",
        "담화분석\n",
        "나는 하늘을 나는 비행기를 보았다. > 나는 하늘에서 날고있는 비행기를 보았다.\n",
        "대화의 맥락과 의도 파악 > 문맥과 의도에 맞게 문장의 의미를 분석하는 과정 \n",
        "\n",
        "우리가 정제를 안해주면 컴퓨터는 모른다.\n",
        "나는 아메리카노 그녀는 라떼. 이런 비유가 있다면,\n",
        "라떼 주세요 하면 그녀 주세요 로 안다.\n",
        "\n",
        "그래서 인공지능에서는 윤리 문제가 아주 중요하다.\n",
        "\n",
        "토큰화 (Tokenizing)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8_YwhpFxgRW",
        "outputId": "ffaccde2-f874-435d-b938-4b114890d43e"
      },
      "source": [
        "pip install NLTK"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from NLTK) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmLzRqblxnqP",
        "outputId": "16e6a03d-95e6-4dce-dda5-49e6059378aa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg') #gutenberg 코퍼스 목록"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-b8egIbx2vB",
        "outputId": "a68b92f1-431b-4502-d2bb-9aeacb01beef"
      },
      "source": [
        "from nltk.corpus import gutenberg #corpus : 말뭉치\n",
        "print(gutenberg.fileids())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE_TEGwJzjqU"
      },
      "source": [
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u0QuanSz4e7",
        "outputId": "b93ecc9c-541e-4529-8a36-63b16855c73c"
      },
      "source": [
        "len(emma)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up6KEeXRzIJD"
      },
      "source": [
        "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL-wayygyWOy",
        "outputId": "f1a72522-d28c-408c-dd07-835973d498d1"
      },
      "source": [
        "emma.concordance(\"surprize\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Displaying 25 of 37 matches:\n",
            "er father , was sometimes taken by surprize at his being still able to pity ` \n",
            "hem do the other any good .\" \" You surprize me ! Emma must do Harriet good : a\n",
            "Knightley actually looked red with surprize and displeasure , as he stood up ,\n",
            "r . Elton , and found to his great surprize , that Mr . Elton was actually on \n",
            "d aid .\" Emma saw Mrs . Weston ' s surprize , and felt that it must be great ,\n",
            "father was quite taken up with the surprize of so sudden a journey , and his f\n",
            "y , in all the favouring warmth of surprize and conjecture . She was , moreove\n",
            "he appeared , to have her share of surprize , introduction , and pleasure . Th\n",
            "ir plans ; and it was an agreeable surprize to her , therefore , to perceive t\n",
            "talking aunt had taken me quite by surprize , it must have been the death of m\n",
            "f all the dialogue which ensued of surprize , and inquiry , and congratulation\n",
            " the present . They might chuse to surprize her .\" Mrs . Cole had many to agre\n",
            "the mode of it , the mystery , the surprize , is more like a young woman ' s s\n",
            " to her song took her agreeably by surprize -- a second , slightly but correct\n",
            "\" \" Oh ! no -- there is nothing to surprize one at all .-- A pretty fortune ; \n",
            "t to be considered . Emma ' s only surprize was that Jane Fairfax should accep\n",
            "of your admiration may take you by surprize some day or other .\" Mr . Knightle\n",
            "ation for her will ever take me by surprize .-- I never had a thought of her i\n",
            " expected by the best judges , for surprize -- but there was great joy . Mr . \n",
            " sound of at first , without great surprize . \" So unreasonably early !\" she w\n",
            "d Frank Churchill , with a look of surprize and displeasure .-- \" That is easy\n",
            "; and Emma could imagine with what surprize and mortification she must be retu\n",
            "tled that Jane should go . Quite a surprize to me ! I had not the least idea !\n",
            " . It is impossible to express our surprize . He came to speak to his father o\n",
            "g engaged !\" Emma even jumped with surprize ;-- and , horror - struck , exclai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbB1kU8y0Rq5"
      },
      "source": [
        "from nltk.corpus import gutenberg"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIr4Omh70Vjq",
        "outputId": "9fdead84-7aab-4f01-dcd5-9e09269b016f"
      },
      "source": [
        "gutenberg.fileids()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk9TVj6K00Of"
      },
      "source": [
        "emma = gutenberg.words('austen-emma.txt')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZM662wZ07ej",
        "outputId": "ebed279c-e3eb-4536-ae0f-e3f00aec8f06"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFikbL1l0YZe",
        "outputId": "81c050b3-ef22-4eee-ccc5-01f6c0b95065"
      },
      "source": [
        "for fileid in gutenberg.fileids():\n",
        "  num_chars = len(gutenberg.raw(fileid))\n",
        "  num_words = len(gutenberg.words(fileid))\n",
        "  num_sents = len(gutenberg.sents(fileid))\n",
        "  num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "  print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 25 26 austen-emma.txt\n",
            "5 26 17 austen-persuasion.txt\n",
            "5 28 22 austen-sense.txt\n",
            "4 34 79 bible-kjv.txt\n",
            "5 19 5 blake-poems.txt\n",
            "4 19 14 bryant-stories.txt\n",
            "4 18 12 burgess-busterbrown.txt\n",
            "4 20 13 carroll-alice.txt\n",
            "5 20 12 chesterton-ball.txt\n",
            "5 23 11 chesterton-brown.txt\n",
            "5 18 11 chesterton-thursday.txt\n",
            "4 21 25 edgeworth-parents.txt\n",
            "5 26 15 melville-moby_dick.txt\n",
            "5 52 11 milton-paradise.txt\n",
            "4 12 9 shakespeare-caesar.txt\n",
            "4 12 8 shakespeare-hamlet.txt\n",
            "4 12 7 shakespeare-macbeth.txt\n",
            "5 36 12 whitman-leaves.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfepxfKY2ytY"
      },
      "source": [
        "**중요한 코드이니 한번 더 쳐보자**\n",
        "\n",
        "리스트를 뽑아서 가져오는 포문이다.\n",
        "\n",
        "    for fileid in gutenberg.fileids():\n",
        "      num_chars = len(gutenberg.raw(fileid))\n",
        "      num_words = len(gutenberg.words(fileid))\n",
        "      num_sents = len(gutenberg.sents(fileid))\n",
        "      num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
        "      print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5VGvW3R0p9Z"
      },
      "source": [
        "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA3gK6Di1FuP",
        "outputId": "0062d3d4-d5c5-443b-e1d1-7e5fbe9e0b36"
      },
      "source": [
        "macbeth_sentences"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBpIQQPN1INb",
        "outputId": "17c585d2-a400-413b-cd92-4d1b1fca5489"
      },
      "source": [
        "macbeth_sentences[1116]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Double',\n",
              " ',',\n",
              " 'double',\n",
              " ',',\n",
              " 'toile',\n",
              " 'and',\n",
              " 'trouble',\n",
              " ';',\n",
              " 'Fire',\n",
              " 'burne',\n",
              " ',',\n",
              " 'and',\n",
              " 'Cauldron',\n",
              " 'bubble']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BiIuma_1LwW"
      },
      "source": [
        "longest_len = max(len(s) for s in macbeth_sentences)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se6AjOoo1P4I"
      },
      "source": [
        "[s for s in macbeth_sentences if len(s) == longest_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYh3jgpi1UEh"
      },
      "source": [
        "hamlet_sents = gutenberg.sents('shakespeare-hamlet.txt')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOtzPpdS2LJ7",
        "outputId": "5f842508-117f-40f0-d5c0-f0654f3e5584"
      },
      "source": [
        "#.sents() 함수를 사용하면 문장별로 토크나이즈를 해준다. \n",
        "hamlet_sents"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']'], ['Actus', 'Primus', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRTCPbOX4fy4"
      },
      "source": [
        "예제를 해보자\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASPKvE42q6N"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogwYpHsT4ist",
        "outputId": "6e026108-10b9-4921-a4d5-6587c7f1dc6e"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8mdOYX84nX6"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "sent = 'we become what we think about'"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxuW0Dbz4xJu"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tagged_list = pos_tag(word_tokenize(sent))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQqA0jL348Wu",
        "outputId": "f9f98993-47b4-4924-97fc-19d32fae5931"
      },
      "source": [
        "tagged_list"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('we', 'PRP'),\n",
              " ('become', 'VBP'),\n",
              " ('what', 'WP'),\n",
              " ('we', 'PRP'),\n",
              " ('think', 'VBP'),\n",
              " ('about', 'IN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3b0AYja5BEP"
      },
      "source": [
        "nltk에 있는 품사 태깅을 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mxPxpXY49W8"
      },
      "source": [
        "from nltk.tag import untag"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H--xBHAf5KYk",
        "outputId": "66c0d6b0-c0b2-40da-f00d-615be9bb331e"
      },
      "source": [
        "untag(tagged_list)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we', 'become', 'what', 'we', 'think', 'about']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0T8nH0R5Ni_"
      },
      "source": [
        "언태그\n",
        "\n",
        "기본 태깅방식이나 옛날 방식"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCvXeWaO5iIi"
      },
      "source": [
        "한글 형태소 분석기 설치 \n",
        "\n",
        "JDK 1.8 설치\n",
        "\n",
        "JPype1 설치 > pip install JPype1\n",
        "\n",
        "koNLPy 설치 > pip install koNLPy\n",
        "\n",
        "코랩이 아니라면 프롬프트를 키고 설치\n",
        "\n",
        "Mecab 이 제일 성능이 좋다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWjLETYw8w0n"
      },
      "source": [
        "임베딩이란?\n",
        "\n",
        "- 범주형 자료를 연속형 벡터 형태로 변환시키는 것\n",
        "\n",
        "- 인간의 언어를 컴퓨터가 이해할 수 있는 언어로 변환\n",
        "\n",
        "- 인간이 이해하는 언어(문자열)을 숫자로 변환하여 벡터 공간에 표현\n",
        "\n",
        "워드 임베딩의 종류\n",
        "\n",
        "1. 빈도 기반 DTM, TF-IDF\n",
        "\n",
        "2. 토픽 기반\n",
        "\n",
        "  주어진 문서에 잠재된 주제를 추론하기 위한 임베딩\n",
        "\n",
        "LDA(Latent Dirichlet Allocation)\n",
        "\n",
        "3. 예측 기반\n",
        "\n",
        "  주어진 문장이나 단어의 다음 단어 예측, 주변 단어에 대한 예측, Masking된 단어의 예측\n",
        "\n",
        "  Word2Vec FastText BERT ELMo GPT\n",
        "\n",
        "  자원소모가 많기 때문에 컴퓨팅 파워가 좋아야함.\n",
        "\n",
        "  단어/문장기반 워드 임베딩\n",
        "\n",
        "  단어기반: Word2Vec FastText Glove\n",
        "\n",
        "  문장기반: ELMo GPT BERT GPT2\n",
        "\n",
        "\n",
        "DTM :\n",
        "\n",
        "- 문서는 단어의 집합으로 이루어져 있다는 개념 적용\n",
        "- 다수의 문서에 등상하는 각 단어 들의 빈도를 행렬 구조로 표현\n",
        "- 각 문서에 주로 사용되는 단어파악 가능\n",
        "\n",
        "빈도수만 나옴, 빈도가 높아서 확인해보니 a, the 같은게 많이나옴\n",
        "\n",
        "TDM :\n",
        "\n",
        "- 단어를 기준으로 (DTM과 반대로)\n",
        "\n",
        "한계: 대부분 값이 0, 일부만 의미있는 값으로 표현됨.\n",
        "\n",
        "단어별 가중치를 부여하지 않음.\n",
        "\n",
        "a, the 같은 중요치 않은 단어가 여러 문서에 공통으로 포함될시 > 유사한 문서로 판단.\n",
        "\n",
        "단어 빈도- 역 문서 빈도 (TF-IDF)\n",
        "\n",
        "TF(Term Frequency)란?\n",
        "\n",
        "- 특정 문서 내 단어가 등장한 빈도\n",
        "\n",
        "DF\n",
        "\n",
        "ex) \n",
        "\n",
        "원자 라는 단어의 경우\n",
        "- 일반적인 문서들 : 자주등장하지 않아 IDF값이 높아지고 문서의 핵심어가됨.\n",
        "- 논문 : 두루 등장하니 상투어가 됨.\n",
        "\n",
        "DF가 0인 경우도 있을수 있으므로\n",
        "\n",
        "IDF = 문서수/1+DF \n",
        "\n",
        "문서수가 많아지면 IDF가 지나치게 커질수 있으므로 전체에 로그를 취해줌.\n",
        "\n",
        "LDA 개요\n",
        "\n",
        "말뭉치로부터 어떤 중요한 주제를 추출하는 기법\n",
        "\n",
        "문서 하나에 어떤 단어가 몇번 나오나\n",
        "\n",
        "추출한 단어로, 단어의 토픽별 확률을 도출\n",
        "\n",
        "토픽별 확률이 높은단어, 이 토픽의 주제이구나 이런식으로 판단.\n",
        "\n",
        "문서별 토픽의 구성을 분석\n",
        "\n",
        "즉, 문서를 단어의 빈도를 토픽에서 뽑고, 문서별 토픽이 차지하는 비중을분석.\n",
        "\n",
        "희소표현과 원핫 인코딩\n",
        "\n",
        "희소표현 : 표현하고자 하는 값만 1\n",
        "\n",
        "원핫 인코딩 : 0과 1로 표현, 특정 단어만 1로 활성화한 벡터로 변환하는방법\n",
        "\n",
        "분산표현과 Word2Vec\n",
        "\n",
        "분산표현 \n",
        "\n",
        "- 분포가설 가정 하에 문자열을 표현하는 방법\n",
        "\n",
        "  분포가설: 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가질 확률이 높다는 가설\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "- 말뭉치(코퍼스)에서 각 단어를 단어의 의미에 따라 여러 차원에 분산하여 벡터로 표현\n",
        "\n",
        "좌표별 밀집도 (유사한 단어가 유사한 위치)\n",
        "\n",
        "단어 벡터간 연산을 통해 의미 파악이 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXNI5EOuHsQg"
      },
      "source": [
        "Word2Vec 에는 CBOW(Continous Bag-of-Word), skip-gram 이 있다.\n",
        "\n",
        "주변단어(좌, 우)를 임베딩하여 중심단어를 예측.\n",
        "\n",
        "중심단어를 임베딩 하여 주변단어를 예측\n",
        "\n",
        "Projection: 중요한 내용을 담은 작은 크기의 벡터.\n",
        "\n",
        "(벡터는 크기와 방향을 가진다.)\n",
        "\n",
        "윈도우: 중심단어를 기준으로 참고하고자 하는 주변 단어의 범위\n",
        "\n",
        "슬라이딩 윈도우\n",
        "\n",
        "- 전체 학습문장에 대해 윈도우를 이동하면서 중심단어와 주변단어 데이터셋을 추출하는 과정\n",
        "\n",
        "Pre-trained 모델 : 대용량 코퍼스 데이터를 이용하여 전에 학습된 모델\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axBiXeCZ5MRi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
